{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/alejandrohernandezmunuera/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "#Local files(same folder)\n",
    "from docker.consumer.modelFunctions import *\n",
    "from docker.producer.readingFunctions import *\n",
    "\n",
    "#Python libraries\n",
    "import timeit\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.model_selection import train_test_split  \n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import svm\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import metrics\n",
    "from sklearn.externals import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataSet = obtain_mailPathList('Data', untar=True, labeled=True)\n",
    "dataSet = obtain_mailPathList('Data', labeled=True)\n",
    "df = pd.DataFrame(data=dataSet, columns=[\"filepath\", \"label\"])\n",
    "\n",
    "#Read mail files\n",
    "df['file'] = df.filepath.map(readFilepath).astype(str)\n",
    "\n",
    "#Divide metadata and content\n",
    "df['pre_info'] = df.file.map(info_part)\n",
    "df['content'] = df.file.map(content_part)\n",
    "\n",
    "#Remove data points without content found\n",
    "#df[df['content']=='']\n",
    "cond = df['content']!=''\n",
    "df = df[cond]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPAM    32928\n",
      "HAM     19088\n",
      "Name: label, dtype: int64\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Check if data is balanced\n",
    "labelCount = df['label'].value_counts()\n",
    "print(labelCount)\n",
    "barplot = (labelCount/labelCount.sum()).plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Randomly discard SPAM datapoints to have a uniform distribution\n",
    "#Tricky step: do not introduce bias\n",
    "\n",
    "#Separate based on label\n",
    "dfS = df[df['label']=='SPAM']\n",
    "dfH = df[df['label']=='HAM']\n",
    "#Subsample the larger set\n",
    "dfSPAMsample = dfS.sample(frac=dfH.size/dfS.size)\n",
    "\n",
    "#Concatenate sets and reset index\n",
    "dfBalance = pd.concat([dfSPAMsample, dfH])\n",
    "dfBalance = dfBalance.reset_index(drop=True)\n",
    "\n",
    "#Show distribution\n",
    "labelCount = dfBalance['label'].value_counts()\n",
    "barplot = (labelCount/labelCount.sum()).plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filepath</th>\n",
       "      <th>label</th>\n",
       "      <th>file</th>\n",
       "      <th>pre_info</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Data/SPAM/GP/part4/msg7799.eml</td>\n",
       "      <td>SPAM</td>\n",
       "      <td>From: \"luckydayvp2004\" &lt;luckydayvp2004@yahoo.c...</td>\n",
       "      <td>From: \"luckydayvp2004\" &lt;luckydayvp2004@yahoo.c...</td>\n",
       "      <td>&lt;!DOCTYPE HTML PUBLIC \"-//W3C//DTD HTML 3.2//E...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Data/SPAM/GP/part2/msg4650.eml</td>\n",
       "      <td>SPAM</td>\n",
       "      <td>From: &lt;exchange-robot@paypal.com&gt;\\nTo: &lt;paliou...</td>\n",
       "      <td>From: &lt;exchange-robot@paypal.com&gt;\\nTo: &lt;paliou...</td>\n",
       "      <td>&lt;!DOCTYPE html PUBLIC \"-//W3C//DTD HTML 4.01 T...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Data/SPAM/BG/2004/10/1099198816.24383_842.txt</td>\n",
       "      <td>SPAM</td>\n",
       "      <td>Return-Path: &lt;qaylct@mails.ch&gt;\\nDelivered-To: ...</td>\n",
       "      <td>Return-Path: &lt;qaylct@mails.ch&gt;\\nDelivered-To: ...</td>\n",
       "      <td>&lt;html&gt;\\n\\t&lt;head&gt;\\n\\t\\t&lt;title&gt;annoy&lt;/title&gt;\\n\\t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Data/SPAM/BG/2004/09/1095715098.11795_203.txt</td>\n",
       "      <td>SPAM</td>\n",
       "      <td>Return-Path: &lt;ppjetcwa@yahoo.com&gt;\\nDelivered-T...</td>\n",
       "      <td>Return-Path: &lt;ppjetcwa@yahoo.com&gt;\\nDelivered-T...</td>\n",
       "      <td>--0-8178544771-9704866217=:63695\\nContent-Type...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Data/SPAM/SH/HP/prodmsg.2.436895.200571</td>\n",
       "      <td>SPAM</td>\n",
       "      <td>Received: from qznet.biz (72.26.221.23)\\n  by ...</td>\n",
       "      <td>Received: from qznet.biz (72.26.221.23)\\n  by ...</td>\n",
       "      <td>This is a multi-part message in MIME format\\n\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38171</td>\n",
       "      <td>Data/HAM/lokay-m/enron_t_s/22</td>\n",
       "      <td>HAM</td>\n",
       "      <td>Message-ID: &lt;13277423.1075844086672.JavaMail.e...</td>\n",
       "      <td>Message-ID: &lt;13277423.1075844086672.JavaMail.e...</td>\n",
       "      <td>Please join me on Thursday, October 26, from 5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38172</td>\n",
       "      <td>Data/HAM/lokay-m/enron_t_s/149</td>\n",
       "      <td>HAM</td>\n",
       "      <td>Message-ID: &lt;7218935.1075854998012.JavaMail.ev...</td>\n",
       "      <td>Message-ID: &lt;7218935.1075854998012.JavaMail.ev...</td>\n",
       "      <td>\\nPlease join Dave Neubauer on Wednesday, Augu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38173</td>\n",
       "      <td>Data/HAM/lokay-m/enron_t_s/171</td>\n",
       "      <td>HAM</td>\n",
       "      <td>Message-ID: &lt;32950741.1075854998588.JavaMail.e...</td>\n",
       "      <td>Message-ID: &lt;32950741.1075854998588.JavaMail.e...</td>\n",
       "      <td>Hey guys, \\n\\nPlease review these and let me k...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38174</td>\n",
       "      <td>Data/HAM/lokay-m/enron_t_s/185</td>\n",
       "      <td>HAM</td>\n",
       "      <td>Message-ID: &lt;21573677.1075854998959.JavaMail.e...</td>\n",
       "      <td>Message-ID: &lt;21573677.1075854998959.JavaMail.e...</td>\n",
       "      <td>Please join me for a floor meeting on June 14,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38175</td>\n",
       "      <td>Data/HAM/lokay-m/enron_t_s/25</td>\n",
       "      <td>HAM</td>\n",
       "      <td>Message-ID: &lt;7115480.1075844086752.JavaMail.ev...</td>\n",
       "      <td>Message-ID: &lt;7115480.1075844086752.JavaMail.ev...</td>\n",
       "      <td>Notice to All ETS Personnel\\n\\n\\n On October 1...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>38176 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            filepath label  \\\n",
       "0                     Data/SPAM/GP/part4/msg7799.eml  SPAM   \n",
       "1                     Data/SPAM/GP/part2/msg4650.eml  SPAM   \n",
       "2      Data/SPAM/BG/2004/10/1099198816.24383_842.txt  SPAM   \n",
       "3      Data/SPAM/BG/2004/09/1095715098.11795_203.txt  SPAM   \n",
       "4            Data/SPAM/SH/HP/prodmsg.2.436895.200571  SPAM   \n",
       "...                                              ...   ...   \n",
       "38171                  Data/HAM/lokay-m/enron_t_s/22   HAM   \n",
       "38172                 Data/HAM/lokay-m/enron_t_s/149   HAM   \n",
       "38173                 Data/HAM/lokay-m/enron_t_s/171   HAM   \n",
       "38174                 Data/HAM/lokay-m/enron_t_s/185   HAM   \n",
       "38175                  Data/HAM/lokay-m/enron_t_s/25   HAM   \n",
       "\n",
       "                                                    file  \\\n",
       "0      From: \"luckydayvp2004\" <luckydayvp2004@yahoo.c...   \n",
       "1      From: <exchange-robot@paypal.com>\\nTo: <paliou...   \n",
       "2      Return-Path: <qaylct@mails.ch>\\nDelivered-To: ...   \n",
       "3      Return-Path: <ppjetcwa@yahoo.com>\\nDelivered-T...   \n",
       "4      Received: from qznet.biz (72.26.221.23)\\n  by ...   \n",
       "...                                                  ...   \n",
       "38171  Message-ID: <13277423.1075844086672.JavaMail.e...   \n",
       "38172  Message-ID: <7218935.1075854998012.JavaMail.ev...   \n",
       "38173  Message-ID: <32950741.1075854998588.JavaMail.e...   \n",
       "38174  Message-ID: <21573677.1075854998959.JavaMail.e...   \n",
       "38175  Message-ID: <7115480.1075844086752.JavaMail.ev...   \n",
       "\n",
       "                                                pre_info  \\\n",
       "0      From: \"luckydayvp2004\" <luckydayvp2004@yahoo.c...   \n",
       "1      From: <exchange-robot@paypal.com>\\nTo: <paliou...   \n",
       "2      Return-Path: <qaylct@mails.ch>\\nDelivered-To: ...   \n",
       "3      Return-Path: <ppjetcwa@yahoo.com>\\nDelivered-T...   \n",
       "4      Received: from qznet.biz (72.26.221.23)\\n  by ...   \n",
       "...                                                  ...   \n",
       "38171  Message-ID: <13277423.1075844086672.JavaMail.e...   \n",
       "38172  Message-ID: <7218935.1075854998012.JavaMail.ev...   \n",
       "38173  Message-ID: <32950741.1075854998588.JavaMail.e...   \n",
       "38174  Message-ID: <21573677.1075854998959.JavaMail.e...   \n",
       "38175  Message-ID: <7115480.1075844086752.JavaMail.ev...   \n",
       "\n",
       "                                                 content  \n",
       "0      <!DOCTYPE HTML PUBLIC \"-//W3C//DTD HTML 3.2//E...  \n",
       "1      <!DOCTYPE html PUBLIC \"-//W3C//DTD HTML 4.01 T...  \n",
       "2      <html>\\n\\t<head>\\n\\t\\t<title>annoy</title>\\n\\t...  \n",
       "3      --0-8178544771-9704866217=:63695\\nContent-Type...  \n",
       "4      This is a multi-part message in MIME format\\n\\...  \n",
       "...                                                  ...  \n",
       "38171  Please join me on Thursday, October 26, from 5...  \n",
       "38172  \\nPlease join Dave Neubauer on Wednesday, Augu...  \n",
       "38173  Hey guys, \\n\\nPlease review these and let me k...  \n",
       "38174  Please join me for a floor meeting on June 14,...  \n",
       "38175  Notice to All ETS Personnel\\n\\n\\n On October 1...  \n",
       "\n",
       "[38176 rows x 5 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Show table content\n",
    "dfBalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('tfidf_vectorizer',\n",
       "                 TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.float64'>,\n",
       "                                 encoding='utf-8', input='content',\n",
       "                                 lowercase=True, max_df=1.0, max_features=None,\n",
       "                                 min_df=1, ngram_range=(1, 1), norm='l2',\n",
       "                                 preprocessor=None, smooth_idf=True,\n",
       "                                 stop_words='english', strip_accents=None,\n",
       "                                 sublinear_tf=False,\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=<function clean_email at 0x7fb1bd4acef0>,\n",
       "                                 use_idf=True, vocabulary=None)),\n",
       "                ('classificador',\n",
       "                 MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Train model: Tf-idf + Bayes\n",
    "\n",
    "#Train-test split\n",
    "X = dfBalance['content']\n",
    "y = dfBalance['label']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3,random_state=101)\n",
    "\n",
    "#TODO: Check other prepocessing options: word2vec, bag of words, doc2vec\n",
    "clf_pipeline = Pipeline([\n",
    "    ('tfidf_vectorizer', TfidfVectorizer(tokenizer=clean_email, stop_words='english')),\n",
    "    ('classificador', MultinomialNB())])\n",
    "\n",
    "#Train\n",
    "clf_pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time spent to predict test set: 124.520981522\n",
      "[[5679   68]\n",
      " [  50 5656]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        SPAM       0.99      0.99      0.99      5747\n",
      "         HAM       0.99      0.99      0.99      5706\n",
      "\n",
      "    accuracy                           0.99     11453\n",
      "   macro avg       0.99      0.99      0.99     11453\n",
      "weighted avg       0.99      0.99      0.99     11453\n",
      "\n",
      "0.9896970226141623\n"
     ]
    }
   ],
   "source": [
    "#Performance on test set\n",
    "\n",
    "#Predict test data\n",
    "tic=timeit.default_timer()\n",
    "\n",
    "y_pred = clf_pipeline.predict(X_test)\n",
    "\n",
    "toc=timeit.default_timer()\n",
    "model1time=toc - tic\n",
    "print(f\"Time spent to predict test set: {model1time}\")#other processes might interfere\n",
    "\n",
    "#Print performance metrics: confusion matrix, precision, recall, f1-score, accuracy\n",
    "def printModelMetrics(label,pred,posLabels):\n",
    "    print(metrics.confusion_matrix(label, pred, labels=posLabels))\n",
    "    print(metrics.classification_report(label, pred, labels=posLabels))\n",
    "    print(metrics.accuracy_score(label, pred))\n",
    "    \n",
    "printModelMetrics(y_test,y_pred,classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-6.969050729176774 tr\n",
      "-6.906825220317894 e\n",
      "-6.874067170667888 content\n",
      "-6.742242096180259 nbsp\n",
      "-6.652834372970496 http\n",
      "-6.61765347849213 html\n",
      "-6.599169997089252 td\n",
      "-6.058163726528697 font\n",
      "-5.595654716940808 d\n",
      "-5.336631058216161 br\n"
     ]
    }
   ],
   "source": [
    "#Model exploration: Find out which words are more important in the classification\n",
    "\n",
    "vectorizer= clf_pipeline.named_steps['tfidf_vectorizer']\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "\n",
    "classifier = clf_pipeline.named_steps['classificador']\n",
    "\n",
    "wNumber = 10\n",
    "top_features = sorted(zip(classifier.coef_[0], feature_names))[-wNumber:]\n",
    "for coef, feat in top_features:\n",
    "        print(coef, feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text:\n",
      "Vasant,\n",
      "\n",
      "Yes, it's perfect. Please, indicate that the wording was unfortunate.\n",
      "\n",
      "Vince\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Vasant Shanbhogue\n",
      "03/08/2001 11:20 AM\n",
      "To: Vince J Kaminski/HOU/ECT@ECT\n",
      "cc:  \n",
      "Subject: RE: Weekly report\n",
      "\n",
      "Hi Vince,\n",
      "\n",
      " regarding David Port's response to Kevin Kindall's email, I feel that I \n",
      "should respond, at least to make our position clear.  Please indicate if the \n",
      "following response is appropriate -------\n",
      "\n",
      "\"Hi David,\n",
      "\n",
      "  I understand that you were slightly upset over a comment Kevin Kindall made \n",
      "in one of his weekly reports.  The intention was never to disparage anybody.  \n",
      "It is just that since Research gets data from a large number of sources, we \n",
      "feel obligated to the data donor to ask any requester for clarification of \n",
      "need.  I completely understand that RAC typically has access to much \n",
      "sensitive information and they have a right to know much information.  We \n",
      "just want to make sure there is open flow of information (it is in \n",
      "everybody's best interests and the company's best interests) and that \n",
      "everybody is aware of how data is flowing.  \n",
      "\n",
      "Best wishes,\n",
      " Vasant\"\n",
      "\n",
      "---------------------- Forwarded by Vasant Shanbhogue/HOU/ECT on 03/08/2001 \n",
      "11:11 AM ---------------------------\n",
      "From: David Port/ENRON@enronXgate on 03/08/2001 08:46 AM\n",
      "To: Kevin Kindall/Corp/Enron@ENRON\n",
      "cc: Vince J Kaminski/HOU/ECT@ECT, Vasant Shanbhogue/HOU/ECT@ECT, Rudi \n",
      "Zipter/ENRON@enronXgate \n",
      "Subject: RE: Weekly report\n",
      "\n",
      "Kevin - thanks for the update.\n",
      "\n",
      "On the Stock option Plans, if your angle is what I suspect, I suggest you get \n",
      "with Rudi Zipter, who has done a great deal of work on this exposure, \n",
      "including characterising and actually booking the short option positions in \n",
      "Enron's equity system. We are already working with Ben Glisan's team firming \n",
      "up a hedging program. We are well advanced in this effort so if you get with \n",
      "my people it could save you a great deal of time.\n",
      "\n",
      "Secondly, I am afraid I do take some exception to your references to Naveen's \n",
      "team in your last point. Generally in RAC I don't believe we are obliged to \n",
      "explain why we need information, except as a courtesy - otherwise that would \n",
      "compromise our role somewhat. Specifically, I am aware of the sensitivity of \n",
      "Raptor, just as I am of the sensitivity of all the information my group is \n",
      "privvy to on a daily basis. Again, we have done a good deal of work on these \n",
      "structures too (I see a position report daily). As I have discussed with \n",
      "Vince, Naveen's request would have been derived from a discussion we all had \n",
      "with Rick, concerning \"meltdown\" scenarios and their effect on, amongst other \n",
      "things, funding vehicles. \n",
      "\n",
      "But I would rather have had a conversation about this than see slightly \n",
      "disparaging remarks about my people in email traffic.\n",
      "\n",
      "Rgds\n",
      "DP \n",
      "\n",
      " -----Original Message-----\n",
      "From:  Kindall, Kevin  \n",
      "Sent: Monday, March 05, 2001 8:19 AM\n",
      "To: Kaminski, Vince; Shanbhogue, Vasant\n",
      "Cc: Port, David\n",
      "Subject: Weekly report\n",
      "\n",
      " << File: March 2, 2001 Report.doc >> \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Predicted label: ['HAM'] with probability 100.00%\n",
      "Real label: HAM\n"
     ]
    }
   ],
   "source": [
    "#Predict specific data point with prob\n",
    "\n",
    "def predict_content(pipeline, text):\n",
    "    pred = pipeline.predict([text])\n",
    "    prob  = np.max(pipeline.predict_proba([text]))\n",
    "    print(\"Text:\")\n",
    "    print(text)\n",
    "    print(\"\\n\")\n",
    "    print(f\"Predicted label: {pred} with probability {prob*100:.2f}%\")\n",
    "\n",
    "\n",
    "indexToPredict = 14\n",
    "predict_content(clf_pipeline, X_test.iloc[indexToPredict])\n",
    "print(f\"Real label: {y_test.iloc[indexToPredict]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text:\n",
      "Vasant,\n",
      "\n",
      "Yes, it's perfect. Please, indicate that the wording was unfortunate.\n",
      "\n",
      "Vince\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Vasant Shanbhogue\n",
      "03/08/2001 11:20 AM\n",
      "To: Vince J Kaminski/HOU/ECT@ECT\n",
      "cc:  \n",
      "Subject: RE: Weekly report\n",
      "\n",
      "Hi Vince,\n",
      "\n",
      " regarding David Port's response to Kevin Kindall's email, I feel that I \n",
      "should respond, at least to make our position clear.  Please indicate if the \n",
      "following response is appropriate -------\n",
      "\n",
      "\"Hi David,\n",
      "\n",
      "  I understand that you were slightly upset over a comment Kevin Kindall made \n",
      "in one of his weekly reports.  The intention was never to disparage anybody.  \n",
      "It is just that since Research gets data from a large number of sources, we \n",
      "feel obligated to the data donor to ask any requester for clarification of \n",
      "need.  I completely understand that RAC typically has access to much \n",
      "sensitive information and they have a right to know much information.  We \n",
      "just want to make sure there is open flow of information (it is in \n",
      "everybody's best interests and the company's best interests) and that \n",
      "everybody is aware of how data is flowing.  \n",
      "\n",
      "Best wishes,\n",
      " Vasant\"\n",
      "\n",
      "---------------------- Forwarded by Vasant Shanbhogue/HOU/ECT on 03/08/2001 \n",
      "11:11 AM ---------------------------\n",
      "From: David Port/ENRON@enronXgate on 03/08/2001 08:46 AM\n",
      "To: Kevin Kindall/Corp/Enron@ENRON\n",
      "cc: Vince J Kaminski/HOU/ECT@ECT, Vasant Shanbhogue/HOU/ECT@ECT, Rudi \n",
      "Zipter/ENRON@enronXgate \n",
      "Subject: RE: Weekly report\n",
      "\n",
      "Kevin - thanks for the update.\n",
      "\n",
      "On the Stock option Plans, if your angle is what I suspect, I suggest you get \n",
      "with Rudi Zipter, who has done a great deal of work on this exposure, \n",
      "including characterising and actually booking the short option positions in \n",
      "Enron's equity system. We are already working with Ben Glisan's team firming \n",
      "up a hedging program. We are well advanced in this effort so if you get with \n",
      "my people it could save you a great deal of time.\n",
      "\n",
      "Secondly, I am afraid I do take some exception to your references to Naveen's \n",
      "team in your last point. Generally in RAC I don't believe we are obliged to \n",
      "explain why we need information, except as a courtesy - otherwise that would \n",
      "compromise our role somewhat. Specifically, I am aware of the sensitivity of \n",
      "Raptor, just as I am of the sensitivity of all the information my group is \n",
      "privvy to on a daily basis. Again, we have done a good deal of work on these \n",
      "structures too (I see a position report daily). As I have discussed with \n",
      "Vince, Naveen's request would have been derived from a discussion we all had \n",
      "with Rick, concerning \"meltdown\" scenarios and their effect on, amongst other \n",
      "things, funding vehicles. \n",
      "\n",
      "But I would rather have had a conversation about this than see slightly \n",
      "disparaging remarks about my people in email traffic.\n",
      "\n",
      "Rgds\n",
      "DP \n",
      "\n",
      " -----Original Message-----\n",
      "From:  Kindall, Kevin  \n",
      "Sent: Monday, March 05, 2001 8:19 AM\n",
      "To: Kaminski, Vince; Shanbhogue, Vasant\n",
      "Cc: Port, David\n",
      "Subject: Weekly report\n",
      "\n",
      " << File: March 2, 2001 Report.doc >> \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Predicted label: ['HAM'] with probability 100.00%\n",
      "Real label: HAM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    }
   ],
   "source": [
    "#Save model to file\n",
    "saveSpamModel(clf_pipeline)\n",
    "\n",
    "#Load from file\n",
    "joblib_model = loadSpamModel()\n",
    "\n",
    "#Use loaded from file\n",
    "predict_content(joblib_model, X_test.iloc[indexToPredict])\n",
    "print(f\"Real label: {y_test.iloc[indexToPredict]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('vectorizer',\n",
       "                 CountVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
       "                                 input='content', lowercase=True, max_df=1.0,\n",
       "                                 max_features=None, min_df=1,\n",
       "                                 ngram_range=(1, 1), preprocessor=None,\n",
       "                                 stop_words='english', strip_accents=None,\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=<function clean_email at 0x7fb1bd4acef0>,\n",
       "                                 vocabulary=None)),\n",
       "                ('classificador',\n",
       "                 SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "                     decision_function_shape='ovr', degree=3,\n",
       "                     gamma='auto_deprecated', kernel='rbf', max_iter=-1,\n",
       "                     probability=False, random_state=None, shrinking=True,\n",
       "                     tol=0.001, verbose=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Train second model: bag of words + SVM\n",
    "\n",
    "#Design pipeline\n",
    "clf_pipeline2 = Pipeline([\n",
    "    ('vectorizer', CountVectorizer(tokenizer=clean_email, stop_words='english')),\n",
    "    #('scaler', StandardScaler()), #TODO: normalize data techniques\n",
    "    ('classificador', svm.SVC())])#svm.SVC(C=0.1, kernel='linear') #TODO: try different kernels\n",
    "\n",
    "#Train model\n",
    "clf_pipeline2.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time spent to predict test set: 226.4817312749999\n",
      "[[4446 1301]\n",
      " [  15 5691]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        SPAM       1.00      0.77      0.87      5747\n",
      "         HAM       0.81      1.00      0.90      5706\n",
      "\n",
      "    accuracy                           0.89     11453\n",
      "   macro avg       0.91      0.89      0.88     11453\n",
      "weighted avg       0.91      0.89      0.88     11453\n",
      "\n",
      "0.8850956081376059\n"
     ]
    }
   ],
   "source": [
    "#Performance on test set\n",
    "\n",
    "#Predict test data\n",
    "tic=timeit.default_timer()\n",
    "\n",
    "y_pred = clf_pipeline2.predict(X_test)\n",
    "\n",
    "toc=timeit.default_timer()\n",
    "model2time= toc - tic\n",
    "print(f\"Time spent to predict test set: {model2time}\")#other processes might interfere\n",
    "\n",
    "#Print metrics\n",
    "printModelMetrics(y_test,y_pred,classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
